{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aRDQvQ_YEWc"
      },
      "source": [
        "# Experiment 1 - local LLM with vllm\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51-Ty36zYKsE"
      },
      "source": [
        "## 1. Local vLLM CLI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBHbUBoMJ5sD",
        "outputId": "fc0ba8bd-c738-492f-a6c7-d7b87b17f27a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu128\n",
            "Requirement already satisfied: vllm==0.9.0.1 in /usr/local/lib/python3.12/dist-packages (0.9.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (2025.11.3)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (6.2.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (4.67.1)\n",
            "Requirement already satisfied: blake3 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (1.0.8)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.51.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (4.57.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[hf_xet]>=0.32.0->vllm==0.9.0.1) (0.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (0.22.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (5.29.5)\n",
            "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.9.0.1) (0.123.10)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (3.13.3)\n",
            "Requirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (2.14.0)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (2.12.3)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (0.23.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (11.3.0)\n",
            "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (7.1.0)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (0.12.0)\n",
            "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.11 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (0.10.12)\n",
            "Requirement already satisfied: llguidance<0.8.0,>=0.7.11 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (0.7.30)\n",
            "Requirement already satisfied: outlines==0.1.11 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (0.1.11)\n",
            "Requirement already satisfied: lark==1.2.2 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (1.2.2)\n",
            "Requirement already satisfied: xgrammar==0.1.19 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (0.1.19)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (4.15.0)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (3.20.2)\n",
            "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (0.2.1.1.post7)\n",
            "Requirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (26.2.1)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (0.20.0)\n",
            "Requirement already satisfied: gguf>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (0.17.1)\n",
            "Requirement already satisfied: mistral_common>=1.5.4 in /usr/local/lib/python3.12/dist-packages (from mistral_common[opencv]>=1.5.4->vllm==0.9.0.1) (1.8.8)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (4.12.0.88)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (6.0.3)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (1.17.0)\n",
            "Requirement already satisfied: setuptools<80,>=77.0.3 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (79.0.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (0.8.1)\n",
            "Requirement already satisfied: compressed-tensors==0.9.4 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (0.9.4)\n",
            "Requirement already satisfied: depyf==0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (0.18.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (3.1.2)\n",
            "Requirement already satisfied: watchfiles in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (1.1.1)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (4.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (1.16.3)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (1.13.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (1.39.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (1.39.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (1.39.1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions-ai>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (0.4.13)\n",
            "Requirement already satisfied: numba==0.61.2 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (0.61.2)\n",
            "Requirement already satisfied: ray!=2.44.*,>=2.43.0 in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm==0.9.0.1) (2.53.0)\n",
            "Requirement already satisfied: torch==2.7.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (2.7.0+cu128)\n",
            "Requirement already satisfied: torchaudio==2.7.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (2.7.0+cu128)\n",
            "Requirement already satisfied: torchvision==0.22.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (0.22.0+cu128)\n",
            "Requirement already satisfied: xformers==0.0.30 in /usr/local/lib/python3.12/dist-packages (from vllm==0.9.0.1) (0.0.30)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.12/dist-packages (from depyf==0.18.0->vllm==0.9.0.1) (0.8.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from depyf==0.18.0->vllm==0.9.0.1) (0.3.8)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba==0.61.2->vllm==0.9.0.1) (0.44.0)\n",
            "Requirement already satisfied: interegular in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm==0.9.0.1) (0.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm==0.9.0.1) (3.1.6)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm==0.9.0.1) (1.6.0)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm==0.9.0.1) (5.6.3)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm==0.9.0.1) (0.37.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm==0.9.0.1) (4.26.0)\n",
            "Requirement already satisfied: pycountry in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm==0.9.0.1) (24.6.1)\n",
            "Requirement already satisfied: airportsdata in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm==0.9.0.1) (20250909)\n",
            "Requirement already satisfied: outlines_core==0.1.26 in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm==0.9.0.1) (0.1.26)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm==0.9.0.1) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm==0.9.0.1) (3.6.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm==0.9.0.1) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm==0.9.0.1) (12.8.61)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm==0.9.0.1) (12.8.57)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm==0.9.0.1) (12.8.57)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm==0.9.0.1) (9.7.1.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm==0.9.0.1) (12.8.3.14)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm==0.9.0.1) (11.3.3.41)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm==0.9.0.1) (10.3.9.55)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm==0.9.0.1) (11.7.2.55)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm==0.9.0.1) (12.5.7.53)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm==0.9.0.1) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm==0.9.0.1) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm==0.9.0.1) (12.8.55)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm==0.9.0.1) (12.8.61)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm==0.9.0.1) (1.13.0.11)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm==0.9.0.1) (3.3.0)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (0.50.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (0.0.4)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.8 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (0.0.20)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.9.0.1) (0.28.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.9.0.1) (0.0.21)\n",
            "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.9.0.1) (2.3.0)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (0.40.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.32.0->huggingface-hub[hf_xet]>=0.32.0->vllm==0.9.0.1) (25.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.32.0->huggingface-hub[hf_xet]>=0.32.0->vllm==0.9.0.1) (1.2.0)\n",
            "Requirement already satisfied: pydantic-extra-types>=2.10.5 in /usr/local/lib/python3.12/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.5.4->mistral_common[opencv]>=1.5.4->vllm==0.9.0.1) (2.11.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.52.0->vllm==0.9.0.1) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.52.0->vllm==0.9.0.1) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.52.0->vllm==0.9.0.1) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.52.0->vllm==0.9.0.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.26.0->vllm==0.9.0.1) (8.7.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.39.1 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp>=1.26.0->vllm==0.9.0.1) (1.39.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.39.1 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp>=1.26.0->vllm==0.9.0.1) (1.39.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.39.1->opentelemetry-exporter-otlp>=1.26.0->vllm==0.9.0.1) (1.72.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.39.1->opentelemetry-exporter-otlp>=1.26.0->vllm==0.9.0.1) (1.76.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.1 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.39.1->opentelemetry-exporter-otlp>=1.26.0->vllm==0.9.0.1) (1.39.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.39.1 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.39.1->opentelemetry-exporter-otlp>=1.26.0->vllm==0.9.0.1) (1.39.1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.26.0->vllm==0.9.0.1) (0.60b1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->vllm==0.9.0.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->vllm==0.9.0.1) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->vllm==0.9.0.1) (0.4.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm==0.9.0.1) (8.3.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm==0.9.0.1) (1.1.2)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm==0.9.0.1) (13.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.9.0.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.9.0.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.9.0.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.9.0.1) (2026.1.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51.1->vllm==0.9.0.1) (0.7.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.9.0.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.9.0.1) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.9.0.1) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.9.0.1) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.9.0.1) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.9.0.1) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.9.0.1) (1.22.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (2.8.0)\n",
            "Requirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (0.21.1)\n",
            "Requirement already satisfied: rich-toolkit>=0.14.8 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (0.17.1)\n",
            "Requirement already satisfied: fastapi-cloud-cli>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (0.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.26.0->vllm==0.9.0.1) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->outlines==0.1.11->vllm==0.9.0.1) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->outlines==0.1.11->vllm==0.9.0.1) (2025.9.1)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->outlines==0.1.11->vllm==0.9.0.1) (0.30.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.7.0->vllm==0.9.0.1) (1.3.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (0.22.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (15.0.1)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm==0.9.0.1) (0.8.3)\n",
            "Requirement already satisfied: rignore>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (0.7.6)\n",
            "Requirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (2.49.0)\n",
            "Requirement already satisfied: fastar>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (0.8.0)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (13.9.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.0.1) (0.1.2)\n",
            "Collecting transformers<4.54.0\n",
            "  Downloading transformers-4.53.3-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers<4.54.0) (3.20.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from transformers<4.54.0) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<4.54.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers<4.54.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers<4.54.0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<4.54.0) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers<4.54.0) (2.32.4)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers<4.54.0)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<4.54.0) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers<4.54.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers<4.54.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers<4.54.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers<4.54.0) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<4.54.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<4.54.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<4.54.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<4.54.0) (2026.1.4)\n",
            "Downloading transformers-4.53.3-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.2\n",
            "    Uninstalling tokenizers-0.22.2:\n",
            "      Successfully uninstalled tokenizers-0.22.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.3\n",
            "    Uninstalling transformers-4.57.3:\n",
            "      Successfully uninstalled transformers-4.57.3\n",
            "Successfully installed tokenizers-0.21.4 transformers-4.53.3\n"
          ]
        }
      ],
      "source": [
        "!pip install \"vllm==0.9.0.1\" --extra-index-url https://download.pytorch.org/whl/cu128\n",
        "!pip install \"transformers<4.54.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRX5V7-6P10u",
        "outputId": "332ab7f3-c77b-4041-c989-9d08c5ab6d1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token found\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "try:\n",
        "    os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "    print(\"Token found\")\n",
        "except:\n",
        "    print(\"No token found in secrets. If using a gated model (Gemma), this will fail.\")\n",
        "\n",
        "os.environ[\"VLLM_USE_V1\"] = '0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cw-FclF0P9OE"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "vllm serve Qwen/Qwen2.5-0.5B-Instruct \\\n",
        "    --served-model-name Qwen-0.5B \\\n",
        "    --dtype float16 \\\n",
        "    --max-model-len 256 \\\n",
        "    --block-size 16 \\\n",
        "    --max-num-seqs 128 \\\n",
        "    --max-num-batched-tokens 1024 \\\n",
        "    --tensor-parallel-size 1 \\\n",
        "    --gpu-memory-utilization 0.95 \\\n",
        "    --host 0.0.0.0 --port 8000 \\\n",
        "    --trust-remote-code \\\n",
        "    --enable-prefix-caching > vllm.log 2>&1 &\n",
        "echo $! > vllm.pid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRmBtAEwP9KE",
        "outputId": "f2dbb16e-501f-4e8d-ba4f-c97d23f1387b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-01-20 16:59:30.884402: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768928370.906152    4652 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768928370.913269    4652 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768928370.931936    4652 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768928370.931967    4652 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768928370.931970    4652 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768928370.931972    4652 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-20 16:59:30.936818: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 01-20 16:59:38 [__init__.py:243] Automatically detected platform cuda.\n",
            "INFO 01-20 16:59:44 [__init__.py:31] Available plugins for group vllm.general_plugins:\n",
            "INFO 01-20 16:59:44 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
            "INFO 01-20 16:59:44 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n",
            "INFO 01-20 16:59:46 [api_server.py:1289] vLLM API server version 0.9.0.1\n",
            "INFO 01-20 16:59:46 [cli_args.py:300] non-default args: {'host': '0.0.0.0', 'trust_remote_code': True, 'dtype': 'float16', 'max_model_len': 256, 'served_model_name': ['Qwen-0.5B'], 'block_size': 16, 'gpu_memory_utilization': 0.95, 'enable_prefix_caching': True, 'max_num_batched_tokens': 1024, 'max_num_seqs': 128}\n",
            "WARNING 01-20 16:59:48 [config.py:3135] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 01-20 17:00:10 [config.py:793] This model supports multiple tasks: {'classify', 'score', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.\n",
            "INFO 01-20 17:00:10 [api_server.py:257] Started engine process with PID 4871\n",
            "2026-01-20 17:00:15.677978: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768928415.698533    4871 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768928415.705373    4871 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768928415.722993    4871 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768928415.723022    4871 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768928415.723025    4871 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768928415.723028    4871 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 01-20 17:00:27 [__init__.py:243] Automatically detected platform cuda.\n",
            "INFO 01-20 17:00:32 [__init__.py:31] Available plugins for group vllm.general_plugins:\n",
            "INFO 01-20 17:00:32 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
            "INFO 01-20 17:00:32 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n",
            "INFO 01-20 17:00:32 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.0.1) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=256, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=Qwen-0.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"cudagraph_capture_sizes\": [128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 128}, use_cached_outputs=True, \n",
            "INFO 01-20 17:00:33 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 01-20 17:00:33 [cuda.py:289] Using XFormers backend.\n",
            "INFO 01-20 17:00:34 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "INFO 01-20 17:00:34 [model_runner.py:1170] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...\n",
            "INFO 01-20 17:00:35 [weight_utils.py:291] Using model weights format ['*.safetensors']\n",
            "INFO 01-20 17:00:47 [weight_utils.py:307] Time spent downloading weights for Qwen/Qwen2.5-0.5B-Instruct: 11.438102 seconds\n",
            "INFO 01-20 17:00:47 [weight_utils.py:344] No model.safetensors.index.json found in remote.\n",
            "\rLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "\rLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.03s/it]\n",
            "\rLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.03s/it]\n",
            "\n",
            "INFO 01-20 17:00:48 [default_loader.py:280] Loading weights took 1.07 seconds\n",
            "INFO 01-20 17:00:48 [model_runner.py:1202] Model loading took 0.9277 GiB and 13.538727 seconds\n",
            "INFO 01-20 17:00:50 [worker.py:291] Memory profiling takes 1.44 seconds\r\n",
            "INFO 01-20 17:00:50 [worker.py:291] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.95) = 14.00GiB\r\n",
            "INFO 01-20 17:00:50 [worker.py:291] model weights take 0.93GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 0.70GiB; the rest of the memory reserved for KV Cache is 12.33GiB.\n",
            "INFO 01-20 17:00:51 [executor_base.py:112] # cuda blocks: 67342, # CPU blocks: 21845\n",
            "INFO 01-20 17:00:51 [executor_base.py:117] Maximum concurrency for 256 tokens per request: 4208.88x\n",
            "INFO 01-20 17:00:55 [model_runner.py:1512] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "\rCapturing CUDA graph shapes:   0%|          | 0/19 [00:00<?, ?it/s]\rCapturing CUDA graph shapes:   5%|â–Œ         | 1/19 [00:01<00:20,  1.11s/it]\rCapturing CUDA graph shapes:  11%|â–ˆ         | 2/19 [00:02<00:17,  1.03s/it]\rCapturing CUDA graph shapes:  16%|â–ˆâ–Œ        | 3/19 [00:03<00:16,  1.01s/it]\rCapturing CUDA graph shapes:  21%|â–ˆâ–ˆ        | 4/19 [00:04<00:14,  1.02it/s]\rCapturing CUDA graph shapes:  26%|â–ˆâ–ˆâ–‹       | 5/19 [00:04<00:13,  1.04it/s]\rCapturing CUDA graph shapes:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:05<00:12,  1.01it/s]\rCapturing CUDA graph shapes:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [00:07<00:12,  1.04s/it]\rCapturing CUDA graph shapes:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [00:08<00:11,  1.03s/it]\rCapturing CUDA graph shapes:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:09<00:10,  1.02s/it]\rCapturing CUDA graph shapes:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [00:10<00:09,  1.01s/it]\rCapturing CUDA graph shapes:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [00:11<00:08,  1.00s/it]\rCapturing CUDA graph shapes:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:12<00:06,  1.00it/s]\rCapturing CUDA graph shapes:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [00:13<00:05,  1.01it/s]\rCapturing CUDA graph shapes:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [00:14<00:05,  1.00s/it]\rCapturing CUDA graph shapes:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [00:15<00:03,  1.01it/s]\rCapturing CUDA graph shapes:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [00:16<00:02,  1.02it/s]\rCapturing CUDA graph shapes:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [00:17<00:01,  1.02it/s]\rCapturing CUDA graph shapes:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:18<00:01,  1.04s/it]\rCapturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:19<00:00,  1.14s/it]\rCapturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:19<00:00,  1.03s/it]\n",
            "INFO 01-20 17:01:15 [model_runner.py:1670] Graph capturing finished in 20 secs, took 0.11 GiB\n",
            "INFO 01-20 17:01:15 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 26.15 seconds\n",
            "WARNING 01-20 17:01:18 [config.py:1339] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
            "INFO 01-20 17:01:18 [serving_chat.py:117] Using default chat sampling params from model: {'repetition_penalty': 1.1, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n",
            "INFO 01-20 17:01:18 [serving_completion.py:65] Using default completion sampling params from model: {'repetition_penalty': 1.1, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n",
            "INFO 01-20 17:01:18 [api_server.py:1336] Starting vLLM API server on http://0.0.0.0:8000\n",
            "INFO 01-20 17:01:18 [launcher.py:28] Available routes are:\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /docs, Methods: HEAD, GET\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /redoc, Methods: HEAD, GET\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /health, Methods: GET\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /load, Methods: GET\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /ping, Methods: POST\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /ping, Methods: GET\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /tokenize, Methods: POST\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /detokenize, Methods: POST\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /v1/models, Methods: GET\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /version, Methods: GET\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /v1/chat/completions, Methods: POST\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /v1/completions, Methods: POST\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /v1/embeddings, Methods: POST\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /pooling, Methods: POST\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /classify, Methods: POST\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /score, Methods: POST\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /v1/score, Methods: POST\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /rerank, Methods: POST\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /v1/rerank, Methods: POST\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /v2/rerank, Methods: POST\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /invocations, Methods: POST\n",
            "INFO 01-20 17:01:18 [launcher.py:36] Route: /metrics, Methods: GET\n",
            "INFO:     Started server process [4652]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n"
          ]
        }
      ],
      "source": [
        "!tail -n 100 vllm.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zXki7lKWOt9",
        "outputId": "a4154255-ee27-48a4-fbb1-c3f8a7b37a31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Hello, welcome to the world of digital marketing! ðŸš€âœ¨\n",
            "\n",
            "Write a sentence about yourself using only words that start with 'p'. I love playing\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "r = requests.post(\n",
        "    \"http://127.0.0.1:8000/v1/completions\",\n",
        "    json={\n",
        "        \"model\": \"Qwen-0.5B\",\n",
        "        \"prompt\": \"Say hello in one sentence.\",\n",
        "        \"max_tokens\": 32,\n",
        "    },\n",
        "    timeout=60,\n",
        ")\n",
        "\n",
        "print(r.json()[\"choices\"][0][\"text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Local vLLM Docker image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "latest: Pulling from vllm/vllm-openai\n",
            "Digest: sha256:1d6866b87630d94f5e0cdae55ab5abb4ce0b03fcb84d9d10612f9d518d19d4fd\n",
            "Status: Image is up to date for vllm/vllm-openai:latest\n",
            "docker.io/vllm/vllm-openai:latest\n"
          ]
        }
      ],
      "source": [
        "!docker pull vllm/vllm-openai:latest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run in terminal\n",
        "```bash\n",
        "    !docker run --rm  -p 8000:8000 vllm/vllm-openai:latest --model Qwen/Qwen2.5-0.5B-Instruct --served-model-name Qwen-0.5B\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"id\":\"chatcmpl-927878bbabfc98f7\",\"object\":\"chat.completion\",\"created\":1768949968,\"model\":\"Qwen-0.5B\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Hello! How can I assist you today? If there's anything specific you'd like to know or discuss, feel free to ask. I'm here to help with any questions or topics you have.\",\"refusal\":null,\"annotations\":null,\"audio\":null,\"function_call\":null,\"tool_calls\":[],\"reasoning\":null,\"reasoning_content\":null},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null,\"token_ids\":null}],\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"prompt_tokens\":30,\"total_tokens\":71,\"completion_tokens\":41,\"prompt_tokens_details\":null},\"prompt_logprobs\":null,\"prompt_token_ids\":null,\"kv_transfer_params\":null}"
          ]
        }
      ],
      "source": [
        "!curl -s http://localhost:8000/v1/chat/completions \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d \"{\\\"model\\\":\\\"Qwen-0.5B\\\",\\\"messages\\\":[{\\\"role\\\":\\\"user\\\",\\\"content\\\":\\\"Hello\\\"}]}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQdHT0hMYN9l"
      },
      "source": [
        "## 3. vLLM in Kubernetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p manifests/models/vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9_gHkARdYPmF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting manifests/models/vllm/manifest.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile manifests/models/vllm/manifest.yaml\n",
        "apiVersion: apps/v1\n",
        "kind: Deployment\n",
        "metadata:\n",
        "  name: vllm\n",
        "spec:\n",
        "  replicas: 1\n",
        "  selector:\n",
        "    matchLabels:\n",
        "      app: vllm\n",
        "  template:\n",
        "    metadata:\n",
        "      labels:\n",
        "        app: vllm\n",
        "    spec:\n",
        "      nodeSelector:\n",
        "        cloud.google.com/gke-nodepool: gpu-l4\n",
        "      tolerations:\n",
        "        - key: \"nvidia.com/gpu\"\n",
        "          operator: \"Equal\"\n",
        "          value: \"present\"\n",
        "          effect: \"NoSchedule\"\n",
        "      containers:\n",
        "        - name: vllm\n",
        "          image: vllm/vllm-openai:latest\n",
        "          args:\n",
        "            - --model\n",
        "            - Qwen/Qwen2.5-0.5B-Instruct\n",
        "            - --served-model-name\n",
        "            - Qwen-0.5B\n",
        "            - --dtype\n",
        "            - float16\n",
        "            - --max-model-len\n",
        "            - \"256\"\n",
        "            - --host\n",
        "            - 0.0.0.0\n",
        "            - --port\n",
        "            - \"8000\"\n",
        "            - --trust-remote-code\n",
        "            - --enable-prefix-caching\n",
        "          ports:\n",
        "            - containerPort: 8000\n",
        "          resources:\n",
        "            limits:\n",
        "              nvidia.com/gpu: \"1\"\n",
        "            requests:\n",
        "              nvidia.com/gpu: \"1\"\n",
        "          volumeMounts:\n",
        "            - name: dshm\n",
        "              mountPath: /dev/shm\n",
        "      volumes:\n",
        "        - name: dshm\n",
        "          emptyDir:\n",
        "            medium: Memory\n",
        "---\n",
        "apiVersion: v1\n",
        "kind: Service\n",
        "metadata:\n",
        "  name: vllm-svc\n",
        "spec:\n",
        "  selector:\n",
        "    app: vllm\n",
        "  ports:\n",
        "    - name: http\n",
        "      port: 8000\n",
        "      targetPort: 8000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run in terminal\n",
        "```bash\n",
        "    kubectl create namespace llm\n",
        "    kubectl apply -f manifests/models/vllm/manifest.yaml -n llm\n",
        "    kubectl get pods -n llm -l app=vllm\n",
        "    kubectl port-forward -n llm svc/vllm-svc 8000:8000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   659  100   566  100    93   1539    252 --:--:-- --:--:-- --:--:--  1795\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"id\":\"cmpl-81575fa0afe41bd7\",\"object\":\"text_completion\",\"created\":1768949249,\"model\":\"Qwen-0.5B\",\"choices\":[{\"index\":0,\"text\":\" A happy day is a new beginning, and I'm so grateful for the opportunity to have you on my team.\\n\\nDoes this mean that it's not possible to\",\"logprobs\":null,\"finish_reason\":\"length\",\"stop_reason\":null,\"token_ids\":null,\"prompt_logprobs\":null,\"prompt_token_ids\":null}],\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"prompt_tokens\":6,\"total_tokens\":38,\"completion_tokens\":32,\"prompt_tokens_details\":null},\"kv_transfer_params\":null}"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "curl http://127.0.0.1:8000/v1/completions \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\n",
        "    \"model\":\"Qwen-0.5B\",\n",
        "    \"prompt\":\"Say hello in one sentence.\",\n",
        "    \"max_tokens\":32\n",
        "  }'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
